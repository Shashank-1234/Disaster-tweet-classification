{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5185cf-c8fd-417d-a405-373068abadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c055426-6089-4a1f-adbb-52211abeef9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ndtv.com/search?searchtext=flood'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndtv_url = \"https://www.ndtv.com/search?searchtext=flood\"\n",
    "ndtv_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63a797dc-0faa-4567-82f7-932e922b198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ndtv_flood.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "\n",
    "# URL for earthquake-related news on NDTV\n",
    "ndtv_url = \"https://www.ndtv.com/search?searchtext=flood\"\n",
    "\n",
    "# Send GET request to fetch the HTML content of the page\n",
    "html_ndtv = requests.get(ndtv_url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "bsobj = soup(html_ndtv.content, 'lxml')\n",
    "\n",
    "# Lists to store the extracted data\n",
    "headlines = []\n",
    "content = []\n",
    "dates = []\n",
    "\n",
    "# Extract the headlines\n",
    "for news in bsobj.findAll('div', {'class': 'src_itm-ttl'}):\n",
    "    headlines.append(news.text.strip())\n",
    "\n",
    "# Extract the content\n",
    "for news1 in bsobj.findAll('div', {'class': 'src_itm-txt'}):\n",
    "    content.append(news1.text.strip())\n",
    "\n",
    "# Extract the dates\n",
    "for news1 in bsobj.findAll('span', {'class': 'src_itm-stx'}):\n",
    "    news_text = news1.text.strip()\n",
    "    date_match = re.search(r'\\b\\w+\\s\\d{1,2},\\s\\d{4}\\b', news_text)\n",
    "    if date_match:\n",
    "        dates.append(date_match.group())\n",
    "\n",
    "# Ensure that the lengths of headlines, content, and dates match\n",
    "min_length = min(len(headlines), len(content), len(dates))\n",
    "headlines = headlines[:min_length]\n",
    "content = content[:min_length]\n",
    "dates = dates[:min_length]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('ndtv_flood.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Headline', 'Content', 'Date'])\n",
    "    for headline, content_text, date in zip(headlines, content, dates):\n",
    "        writer.writerow([headline, content_text, date])\n",
    "\n",
    "print(\"Data has been written to ndtv_flood.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb200b35-d27c-4f4c-901a-fee1bc198d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ndtv_flood.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "\n",
    "# URL for flood-related news on NDTV\n",
    "ndtv_url = \"https://www.ndtv.com/search?searchtext=flood\"\n",
    "\n",
    "# Send GET request to fetch the HTML content of the page\n",
    "html_ndtv = requests.get(ndtv_url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "bsobj = soup(html_ndtv.content, 'lxml')\n",
    "\n",
    "# Lists to store the extracted data\n",
    "headlines = []\n",
    "content = []\n",
    "dates = []\n",
    "links = []\n",
    "\n",
    "# Extract the headlines and links\n",
    "for news in bsobj.findAll('div', {'class': 'src_itm-ttl'}):\n",
    "    headlines.append(news.text.strip())\n",
    "    link_tag = news.find('a', href=True)\n",
    "    href = link_tag['href'] if link_tag else None\n",
    "    links.append(href)\n",
    "\n",
    "# Extract the content\n",
    "for news1 in bsobj.findAll('div', {'class': 'src_itm-txt'}):\n",
    "    content.append(news1.text.strip())\n",
    "\n",
    "# Extract the dates\n",
    "for news1 in bsobj.findAll('span', {'class': 'src_itm-stx'}):\n",
    "    news_text = news1.text.strip()\n",
    "    date_match = re.search(r'\\b\\w+\\s\\d{1,2},\\s\\d{4}\\b', news_text)\n",
    "    if date_match:\n",
    "        dates.append(date_match.group())\n",
    "\n",
    "# Ensure that the lengths of headlines, content, dates, and links match\n",
    "min_length = min(len(headlines), len(content), len(dates), len(links))\n",
    "headlines = headlines[:min_length]\n",
    "content = content[:min_length]\n",
    "dates = dates[:min_length]\n",
    "links = links[:min_length]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('ndtv_flood.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Headline', 'Content', 'Date', 'Link'])\n",
    "    for headline, content_text, date, link in zip(headlines, content, dates, links):\n",
    "        writer.writerow([headline, content_text, date, link])\n",
    "\n",
    "print(\"Data has been written to ndtv_flood.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ae8ba-9193-44e6-b724-43b569841915",
   "metadata": {},
   "source": [
    "NDTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cb646-9376-454f-bd11-0a6d22abef35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae182c-37f2-4c73-8291-013b7ada0d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e30600f6-1a24-4b49-9340-ce9a394d3239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ndtv_earthquake.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "\n",
    "# URL for earthquake-related news on NDTV\n",
    "ndtv_earthquake_url = \"https://www.ndtv.com/search?searchtext=earthquake\"\n",
    "\n",
    "# Send GET request to fetch the HTML content of the page\n",
    "html_ndtv_earthquake = requests.get(ndtv_earthquake_url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "bsobj = soup(html_ndtv_earthquake.content, 'lxml')\n",
    "\n",
    "# Lists to store the extracted data\n",
    "headlines = []\n",
    "content = []\n",
    "dates = []\n",
    "links = []\n",
    "\n",
    "# Extract the headlines and links\n",
    "for news in bsobj.findAll('div', {'class': 'src_itm-ttl'}):\n",
    "    headlines.append(news.text.strip())\n",
    "    link_tag = news.find('a', href=True)\n",
    "    href = link_tag['href'] if link_tag else None\n",
    "    links.append(href)\n",
    "\n",
    "# Extract the content\n",
    "for news1 in bsobj.findAll('div', {'class': 'src_itm-txt'}):\n",
    "    content.append(news1.text.strip())\n",
    "\n",
    "# Extract the dates\n",
    "for news1 in bsobj.findAll('span', {'class': 'src_itm-stx'}):\n",
    "    news_text = news1.text.strip()\n",
    "    date_match = re.search(r'\\b\\w+\\s\\d{1,2},\\s\\d{4}\\b', news_text)\n",
    "    if date_match:\n",
    "        dates.append(date_match.group())\n",
    "\n",
    "# Ensure that the lengths of headlines, content, dates, and links match\n",
    "min_length = min(len(headlines), len(content), len(dates), len(links))\n",
    "headlines = headlines[:min_length]\n",
    "content = content[:min_length]\n",
    "dates = dates[:min_length]\n",
    "links = links[:min_length]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('ndtv_earthquake.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Headline', 'Content', 'Date', 'Link'])\n",
    "    for headline, content_text, date, link in zip(headlines, content, dates, links):\n",
    "        writer.writerow([headline, content_text, date, link])\n",
    "\n",
    "print(\"Data has been written to ndtv_earthquake.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814218ad-c209-4fc9-b2cc-f21415e85841",
   "metadata": {},
   "source": [
    "TimesNow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca09dc91-ae30-4d65-8c72-5414ed5cbbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to timesnow_earthquake.csv\n",
      "Data has been written to timesnow_flood.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import csv\n",
    "\n",
    "# URLs for earthquake and flood news\n",
    "tn_url1 = \"https://www.timesnownews.com/search-result/earthquake%20india\"\n",
    "tn_url2 = \"https://www.timesnownews.com/search-result/flood\"\n",
    "\n",
    "# Send GET requests to fetch the HTML content of the pages\n",
    "html_tn1 = requests.get(tn_url1)\n",
    "html_tn2 = requests.get(tn_url2)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "bsobj_tn1 = soup(html_tn1.content, 'lxml')\n",
    "bsobj_tn2 = soup(html_tn2.content, 'lxml')\n",
    "\n",
    "def extract_and_write_to_csv(bsobj, filename):\n",
    "    # Lists to store the extracted data\n",
    "    texts = []\n",
    "    hrefs = []\n",
    "\n",
    "    # Iterate through each div with class '_3dRe'\n",
    "    for news in bsobj.findAll('div', {'class': '_3dRe'}):\n",
    "        # Extract the text inside the div\n",
    "        text = news.text.strip()\n",
    "        \n",
    "        # Find the 'a' tag within the div\n",
    "        link_tag = news.find('a', href=True)\n",
    "        \n",
    "        # Extract the href attribute if it exists\n",
    "        href = link_tag['href'] if link_tag else None\n",
    "        \n",
    "        # Append the extracted text and href to the lists\n",
    "        texts.append(text)\n",
    "        hrefs.append(href)\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Text', 'Link'])  # Write the header row\n",
    "        for text, href in zip(texts, hrefs):\n",
    "            writer.writerow([text, href])\n",
    "\n",
    "    print(f\"Data has been written to {filename}\")\n",
    "\n",
    "# Extract and write earthquake-related news to a CSV file\n",
    "extract_and_write_to_csv(bsobj_tn1, 'timesnow_earthquake.csv')\n",
    "\n",
    "# Extract and write flood-related news to a separate CSV file\n",
    "extract_and_write_to_csv(bsobj_tn2, 'timesnow_flood.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1d4301d-9a8c-4ac6-8e30-fc042416f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# URL for flood-related news on India Today\n",
    "it_url = \"https://www.indiatoday.in/search/flood\"\n",
    "\n",
    "# Send GET request to fetch the HTML content of the page\n",
    "html = requests.get(it_url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "bsobj = soup(html.content, 'lxml')\n",
    "\n",
    "# Iterate through each div with class 'B1S3_content__wrap__9mSB6'\n",
    "for container in bsobj.findAll('div', {'class': 'B1S3_content__wrap__9mSB6'}):\n",
    "    # Within each div, find the h3 element\n",
    "    h3_tag = container.find('h3', 'class')\n",
    "    if h3_tag:\n",
    "        # Extract the text inside the h3\n",
    "        text = h3_tag.text.strip()\n",
    "        \n",
    "        # Find the 'a' tag within the h3 to get the href link\n",
    "        link_tag = h3_tag.find('a', href=True)\n",
    "        \n",
    "        # Extract the href attribute if it exists\n",
    "        href = link_tag['href'] if link_tag else None\n",
    "        \n",
    "        # Print the text and the href link\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Link: {href}\")\n",
    "        print(\"-\" * 40)  # Separator for clarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0935059-7749-4d53-bde4-3160aea409ce",
   "metadata": {},
   "source": [
    "India Today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829458e1-9f75-49ae-845f-a2ddc279ec86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to 'indiatoday_flood.csv'.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# Initialize the WebDriver (make sure you have the appropriate WebDriver installed, e.g., chromedriver)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the India Today flood search page\n",
    "driver.get(\"https://www.indiatoday.in/search/flood\")\n",
    "\n",
    "# Wait explicitly for the content to load by waiting for a known element on the page\n",
    "try:\n",
    "    element_present = EC.presence_of_element_located((By.CLASS_NAME, 'B1S3_content__wrap__9mSB6'))\n",
    "    WebDriverWait(driver, 20).until(element_present)\n",
    "except Exception as e:\n",
    "    print(\"The content did not load as expected.\")\n",
    "    driver.quit()\n",
    "\n",
    "# Get the page source after JavaScript has executed\n",
    "html = driver.page_source\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "bsobj = soup(html, 'lxml')\n",
    "\n",
    "# List to store the extracted data\n",
    "data = []\n",
    "\n",
    "# Iterate through each div with the articles\n",
    "for container in bsobj.findAll('div', {'class': 'B1S3_content__wrap__9mSB6'}):\n",
    "    # Find the 'a' tag within the div to get the href link and text\n",
    "    link_tag = container.find('a', href=True)\n",
    "    \n",
    "    if link_tag:\n",
    "        # Extract the href attribute\n",
    "        href = link_tag['href']\n",
    "        \n",
    "        # Extract the text inside the 'a' tag\n",
    "        text = link_tag.text.strip()\n",
    "\n",
    "        # Extract the date from the href using regex\n",
    "        date_match = re.search(r'\\d{4}-\\d{2}-\\d{2}', href)\n",
    "        date = date_match.group() if date_match else 'No date found'\n",
    "        \n",
    "        # Append the data to the list\n",
    "        data.append([text, href, date])\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('indiatoday_flood.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Text', 'Link', 'Date'])  # Write the header row\n",
    "    writer.writerows(data)  # Write the data rows\n",
    "\n",
    "print(\"Data has been written to 'indiatoday_flood.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e0128-2eb6-4b98-9776-8bc26a65909d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cecf1446-6b6c-48de-b9bb-9b2be77c7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to 'indiatoday_earthquake.csv'.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# Initialize the WebDriver (make sure you have the appropriate WebDriver installed, e.g., chromedriver)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the India Today flood search page\n",
    "driver.get(\"https://www.indiatoday.in/search/earthquake\")\n",
    "\n",
    "# Wait explicitly for the content to load by waiting for a known element on the page\n",
    "try:\n",
    "    element_present = EC.presence_of_element_located((By.CLASS_NAME, 'B1S3_content__wrap__9mSB6'))\n",
    "    WebDriverWait(driver, 20).until(element_present)\n",
    "except Exception as e:\n",
    "    print(\"The content did not load as expected.\")\n",
    "    driver.quit()\n",
    "\n",
    "# Get the page source after JavaScript has executed\n",
    "html = driver.page_source\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "bsobj = soup(html, 'lxml')\n",
    "\n",
    "# List to store the extracted data\n",
    "data = []\n",
    "\n",
    "# Iterate through each div with the articles\n",
    "for container in bsobj.findAll('div', {'class': 'B1S3_content__wrap__9mSB6'}):\n",
    "    # Find the 'a' tag within the div to get the href link and text\n",
    "    link_tag = container.find('a', href=True)\n",
    "    \n",
    "    if link_tag:\n",
    "        # Extract the href attribute\n",
    "        href = link_tag['href']\n",
    "        \n",
    "        # Extract the text inside the 'a' tag\n",
    "        text = link_tag.text.strip()\n",
    "\n",
    "        # Extract the date from the href using regex\n",
    "        date_match = re.search(r'\\d{4}-\\d{2}-\\d{2}', href)\n",
    "        date = date_match.group() if date_match else 'No date found'\n",
    "        \n",
    "        # Append the data to the list\n",
    "        data.append([text, href, date])\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('indiatoday_earthquake.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Text', 'Link', 'Date'])  # Write the header row\n",
    "    writer.writerows(data)  # Write the data rows\n",
    "\n",
    "print(\"Data has been written to 'indiatoday_earthquake.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322d4da-02f9-4561-97ad-646e8f7d5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize the WebDriver (make sure you have the appropriate WebDriver installed, e.g., chromedriver)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the Twitter search page\n",
    "driver.get(\"https://x.com/search?q=flood%20india&src=typed_query&f=top\")\n",
    "\n",
    "# Wait for the user to manually log in\n",
    "input(\"Please log in to Twitter and then press Enter to continue...\")\n",
    "\n",
    "# Wait for the tweets to load after login\n",
    "try:\n",
    "    # This will wait until at least one tweet is visible\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"article\"))\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"The content did not load as expected.\")\n",
    "    driver.quit()\n",
    "\n",
    "# Extract the tweets\n",
    "tweets = driver.find_elements(By.CSS_SELECTOR, \"article\")\n",
    "\n",
    "for tweet in tweets:\n",
    "    # Print the text of the tweet\n",
    "    print(tweet.text)\n",
    "    print(\"-\" * 40)  # Separator for clarity\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e0218-19d4-4101-b420-4d18bf7b1c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
